	1. One user
	2. 10 user
	3. 100  user
	4. 1000 user
	5. Users > 10,000s - 100,000s
	6. Users > 500,000+
	7. Users > 1,000,000+
	8. Users > 10,000,000+
	9. 10 million user
	10. >10 million user
Some of the interesting takeaways:
• Start with SQL and only move to NoSQL when necessary.
• A consistent theme is take components and separate them out. This allows those components to scale and fail independently. It applies to breaking up tiers and creating microservices.
• Only invest in tasks that differentiate you as a business, don't reinvent the wheel.
• Scalability and redundancy are not two separate concepts, you can often do both at the same time.





Scaling strategy 2
	1. Single server
	2. Scale veritically 
	3. Scale horizontally with load balancer
	4. Resource Intensive Processing separetly
	5. Asynchronous Queue Based Processing
	6. Scaling Beyond
	7. We are reaching the limits of our web server driven, load balanced, queue based asynchronous system. What next?
	8. Search for DNS round robin and other techniques of scaling if you are interested to know more.
	
	
	The 7 Stages Of Scaling Web Apps
	
	• Stage 1 - The Beginning: Simple architecture, low complexity. no redundancy. Firewall, load balancer, a pair of web servers, database server, and internal storage.
	• Stage 2 - More of the same, just bigger.
	• Stage 3 - The Pain Begins: publicity hits. Use reverse proxy, cache static content, load balancers, more databases, re-coding.
	• Stage 4 - The Pain Intensifies: caching with memcached, writes overload and replication takes too long, start database partitioning, shared storage makes sense for content, significant re-architecting for DB.
	• Stage 5 - This Really Hurts!: rethink entire application, partition on geography user ID, etc, create user clusters, using hashing scheme for locating which user belongs to which cluster.
	• Stage 6 - Getting a little less painful: scalable application and database architecture, acceptable performance, starting to add ne features again, optimizing some code, still growing but manageable.
	• Stage 7 - Entering the unknown: where are the remaining bottlenecks (power, space, bandwidth, CDN, firewall, load balancer, storage, people, process, database), all eggs in one basked (single datacenter, single instance of data).
	
	
	
	You may not agree with everything, but there's a lot of useful advice. Here's a summary of some of what is covered:
	• Benchmarking
	• Vertical scaling sucks.
	• Horizontal scaling rocks.
	• Run many application servers
	• Don't keep state in the app server
	• Be stateless
	• Optimization is necessary, but is different than scalability.
	• Cache things you hit all the time.
	• Measure, don't assume, check.
	• Make pages static.
	• Caching is a trade-off.
	• Cache full pages.
	• Cache partial pages.
	• Cache complex data.
	• MySQL query cache is flushed on update.
	• Cache invalidation is hard.
	• Replication scales reads, not writes.
	• Partition to scale writes. 96% of applications can skip this step.
	• Master-master setup facilitates on-line schema changes.
	• Create summary tables and summary databases rather than do COUNT and GROUP-BY at runtime.
	• Make code idempotent. If it fails you should just be able to run it again.
	• Load data asynchronously. Aggregate updates into batches.
	• Move processing to application and out of the database as much as possible.
	• Stored procedures are dangerous.
	• Add more memory.
	• Enable query logging and take a look at what your app is doing.
	• Run different MySQL instances for different work loads.
	• Config tuning helps, query tuning works.
	• Reconsider persistent DB connections.
	• Don't overwork the database. It's hard to scale.
	• Work in parallel.
	• Use a job queuing system.
	• Log http requests.
	• Use light processes for light tasks.
	• Build on APIs internally. Clean loosely coupled APIs are easy to scale.
	• Don't incur technical debt.
	• Automatically handle failures.
	• Make services that always work.
	• Load balancing is the key to horizontal scaling.
	• Redundancy is not load-balancing. Always have n+1 capacity.
	• Plan for disasters.
	• Make backups.
	• Keep software deployments easy.
	• Have everything scripted.
	• Monitor everything. Graph everything.
	• Run one service per server.
	• Don't ever swap memory for disk.
	• Run memcached if you have extra memory.
	• Use memory to save CPU or IO. Balance memory vs CPU vs IO.
	• Netboot your application servers.
	• There's lot of good slides on what to graph.
	• Use a CDN.
	• Use YSlow to find client side problems.
	
